{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://github.com/theonaunheim\">\n",
    "    <img style=\"border-radius: 100%; float: right;\" src=\"static/strawberry_thief_square.png\" width=10% alt=\"Theo Naunheim's Github\">\n",
    "</a>\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "<hr>\n",
    "<br>\n",
    "<h1 align='center'>Preprocessing</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: table; width: 100%\">\n",
    "    <div style=\"display: table-row; width: 100%;\">\n",
    "        <div style=\"display: table-cell; width: 50%; vertical-align: middle;\">\n",
    "            <img src=\"static/log_transform.svg\" width=\"200\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 10%\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 40%; vertical-align: top;\">\n",
    "            <blockquote>\n",
    "                <p style=\"font-style: italic;\">\"Give me six hours to chop down a tree and I will spend the first four sharpening the axe.\"</p>\n",
    "                <br>\n",
    "                <p>-Abraham Lincoln</p>\n",
    "            </blockquote>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:Population_vs_area.svg'>Skbkekas</a> under the <a href='https://creativecommons.org/licenses/by-sa/3.0/deed.en'>CC BY-SA 3.0</a>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the things.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generally\n",
    "\n",
    "Good data will beat good algorithms 9 times out of 10. ML algorithms are finicky beasts, and if you do not adequately clean and prepare your data your results will be garbage. In this section, we're going to run through some of the more useful data processing operations and also take some detours into related data concepts.\n",
    "\n",
    "Outside of the pandas library, most of our preprocessing will come from the following modules:\n",
    "\n",
    "* [sklearn.preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "* [sklearn.model_selection](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "* [sklearn.feature_selection](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)\n",
    "* [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction)\n",
    "* [sklearn.decomposition](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)\n",
    "\n",
    "\n",
    "For additional detail on cleaning data, see the \"Automation and Cleaning Data in Python\" presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Train test split\n",
    "\n",
    "Though it's more of a model validation strategy, [train test splits](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets) are done prior to model creation which is why we will address them here.\n",
    "\n",
    "* [Train Test Split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "As mentioned in the last section, we **train** our algorithm using data prior predicting with it. We can then **test** the predictive power of our model (i.e. how good or bad it is at a particular job) by validating or analyzing its results. The data you use for training and testing should be separate--otherwise your model will be able to perfectly score what it has seen, but won't be able to predict anything useful for unseen data. This is known as **[overfitting](https://en.wikipedia.org/wiki/Overfitting)**.\n",
    "\n",
    "In other words, you take a large portion of your data, and use it to train your model. You then use the remaining data to test on. Obviously this means that you are taking away training data from your model, which can lead to inferior results. We will discuss cross-validation as a strategy to mitigate this issue in the validation notebook.\n",
    "\n",
    "For an excellent demo, see [Scikit: Underfitting vs. Overfitting](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)\n",
    "\n",
    "### If you learn nothing else, learn that you should always segregate your training and testing data.\n",
    "\n",
    "Note: I generally leave the train test split ratio at .25, but [opinions differ on the optimal split](https://stats.stackexchange.com/questions/113994/how-to-choose-the-training-cross-validation-and-test-set-sizes-for-small-sampl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So what's the best way to split into testing and training sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X\n",
      "Iago          5\n",
      "Cassio        0\n",
      "Desdemona    15\n",
      "Bianca       10\n",
      "Name: experience, dtype: int64\n",
      "\n",
      "Train y\n",
      "Iago          66600\n",
      "Cassio        60000\n",
      "Desdemona    210000\n",
      "Bianca        80000\n",
      "Name: salary, dtype: int64\n",
      "\n",
      "Orig df\n",
      "Cassio        0\n",
      "Iago          5\n",
      "Bianca       10\n",
      "Desdemona    15\n",
      "Othello      20\n",
      "Name: experience, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('data/othello.csv', index_col=0)\n",
    "\n",
    "# How to split randomly split a dataset in one line\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['experience'], df['salary'], test_size=.2)\n",
    "\n",
    "print('Train X')\n",
    "print(X_train)\n",
    "print()\n",
    "\n",
    "print('Train y')\n",
    "print(y_train)\n",
    "print()\n",
    "\n",
    "print('Orig df')\n",
    "print(df['experience'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Whether you see it or not, machine learning algorithms work entirely in the realm of numbers. Many machine learning algorithms (i.e. all of them except for decision trees) need to be given \"scaled\" values. This is referred to as [\"feature scaling\" or \"normalization\"](https://en.wikipedia.org/wiki/Feature_scaling). If algorithms aren't given scaled features, they are likely to give you garbage because they will attribute extra importance to the larger values.\n",
    "\n",
    "We generally do this via [Z-scores](https://en.wikipedia.org/wiki/Standard_score) and outlier removal. Your workhorses here will be the StandardScaler and RobustScaler. These are largely the same, but the RobustScaler has better support for outlier smoothing.\n",
    "\n",
    "* [RobustScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)\n",
    "* [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -0.13947001]\n",
      " [-0.5        -0.09344491]\n",
      " [ 0.          0.        ]\n",
      " [ 0.5         0.90655509]\n",
      " [ 1.          1.53417015]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "      <th>robust_experience_z</th>\n",
       "      <th>robust_salary_z</th>\n",
       "      <th>standard_experience_z</th>\n",
       "      <th>standard_salary_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cassio</th>\n",
       "      <td>0.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.139470</td>\n",
       "      <td>-1.414214</td>\n",
       "      <td>-0.870257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iago</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66600.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.093445</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.801322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bianca</th>\n",
       "      <td>10.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.661362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desdemona</th>\n",
       "      <td>15.0</td>\n",
       "      <td>210000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.906555</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.696457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Othello</th>\n",
       "      <td>20.0</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.534170</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.636485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           experience    salary  robust_experience_z  robust_salary_z  \\\n",
       "Cassio            0.0   60000.0                 -1.0        -0.139470   \n",
       "Iago              5.0   66600.0                 -0.5        -0.093445   \n",
       "Bianca           10.0   80000.0                  0.0         0.000000   \n",
       "Desdemona        15.0  210000.0                  0.5         0.906555   \n",
       "Othello          20.0  300000.0                  1.0         1.534170   \n",
       "\n",
       "           standard_experience_z  standard_salary_z  \n",
       "Cassio                 -1.414214          -0.870257  \n",
       "Iago                   -0.707107          -0.801322  \n",
       "Bianca                  0.000000          -0.661362  \n",
       "Desdemona               0.707107           0.696457  \n",
       "Othello                 1.414214           1.636485  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our scalers.\n",
    "rs = RobustScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "# This needs to be fitted and transformed as it is used to convert.\n",
    "print(rs.fit_transform(df))\n",
    "\n",
    "# rs.fit_transform(df) is the same as:\n",
    "# rs.fit(df)\n",
    "# rs.transform(df)\n",
    "\n",
    "# Display nicely.\n",
    "pd.DataFrame(\n",
    "    np.concatenate([\n",
    "        df.values,\n",
    "        rs.fit_transform(df),\n",
    "        ss.fit_transform(df)\n",
    "    ], axis=1),\n",
    "    index=df.index,\n",
    "    columns=[\n",
    "        'experience',\n",
    "        'salary',\n",
    "        'robust_experience_z',\n",
    "        'robust_salary_z',\n",
    "        'standard_experience_z',\n",
    "        'standard_salary_z'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it's easier to keep track of, you can also just do this inside the dataframe.\n",
    "df['salary'] = ss.fit_transform(df[['salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Imputing\n",
    "\n",
    "Did I mention that ML algorithms are tempermental yet? They also don't like missing values. Or positive infinite values. Or negative infinite values. Or anything larger than a 64-bit float, honestly. Your average ML algorithm will throw a fit if you give it a null value. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input contains NaN, infinity or a value too large for dtype('float64').\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'weight': [1, np.NaN, 5, 2, 4],\n",
    "    'height': [np.NaN, 3, 1, np.NaN, 8]\n",
    "})\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "try:\n",
    "    # This is a model so it uses fit() and predict().\n",
    "    svc.fit(\n",
    "        df['weight'].values.reshape(-1,1), \n",
    "        df['height'].values.reshape(-1,1)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How do we get around this? We either drop all the non-compliant values using [DataFrame.dropna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html), replace the values a specified value using [DataFrame.fillna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) or we replace or **impute** a value to the observation. Sklearn has a handy Imputer class to handle this last case.\n",
    "\n",
    "* [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5, 1. , 2.5, 3. , 2.5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputer does a fit() then a transform() to convert the values.\n",
    "imp = Imputer(strategy='median', missing_values=np.NaN, axis=1)\n",
    "imp.fit_transform([[np.NaN, 1.0, 2.5, 3.0, np.NaN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weight  height\n",
       "2     5.0     1.0\n",
       "4     4.0     8.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropna\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weight  height\n",
       "0     1.0     0.0\n",
       "1     0.0     3.0\n",
       "2     5.0     1.0\n",
       "3     2.0     0.0\n",
       "4     4.0     8.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.fillna(df['height'].max()) # can be arbitrary\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## One-Hot Encoding & Label Encoding\n",
    "\n",
    "Whether it's immediately apparent or not, ML algorithms use numbers and only numbers. It may look like 'setosa' or 'virginica' to you, but to the computer it's 0 or 1. While a lot of the time you can simply supply strings to machine learning algorithms and have them do all the transformational nastiness automatically, sometimes you have to get your hands dirty and [do it yourself](http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features). To that end, we're going to take a look at LabelEncoder and OneHotEncoder.\n",
    "\n",
    "* [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "* [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n",
    "\n",
    "The label encoder takes your cateogires and transforms them into simple numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our data.\n",
    "df = pd.DataFrame({\n",
    "    'type': ['puffin', 'seagull', 'puffin', 'owl', 'hermit crab'],\n",
    "    'weight': [4, 1, 4, 3, 2]\n",
    "})\n",
    "\n",
    "# Create our encoders.\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our number-ified categories are: [2 3 2 1 0]\n",
      "Which corresponds with:['puffin' 'seagull' 'puffin' 'owl' 'hermit crab']\n"
     ]
    }
   ],
   "source": [
    "# We are putting in values and changing them, so we transform\n",
    "results = le.fit_transform(df['type'].values)\n",
    "\n",
    "print('Our number-ified categories are: ', end='')\n",
    "print(results)\n",
    "print('Which corresponds with:', end='')\n",
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "print(le.inverse_transform(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You could feed this directly to your ML algorithm, BUT it would be would view an owl (1) as closer to a hermit crab (0) than to a seagull (3). Viewing these as numbers can be problematic, so it makes more sense to turn them into separate binary dimensions like \"is a seagull\", \"is a crab\", \"is an owl\". Also, note that LabelEncoder is ordered based on string name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>weight</th>\n",
       "      <th>hermit crab</th>\n",
       "      <th>owl</th>\n",
       "      <th>puffin</th>\n",
       "      <th>seagull</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>puffin</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seagull</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>puffin</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owl</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hermit crab</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type  weight  hermit crab    owl  puffin  seagull\n",
       "0       puffin       4        False  False    True    False\n",
       "1      seagull       1        False  False   False     True\n",
       "2       puffin       4        False  False    True    False\n",
       "3          owl       3        False   True   False    False\n",
       "4  hermit crab       2         True  False   False    False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We fit and transform because this is a converter.\n",
    "one_hot_data = ohe.fit_transform(results.reshape(-1,1))\n",
    "\n",
    "# Put in a dataframe and make boolean\n",
    "binary_df = pd.DataFrame(\n",
    "    one_hot_data,\n",
    "    columns=le.classes_\n",
    ").astype(bool)\n",
    "\n",
    "# Put it all together.\n",
    "pd.concat([df, binary_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hermit crab</th>\n",
       "      <th>owl</th>\n",
       "      <th>puffin</th>\n",
       "      <th>seagull</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hermit crab    owl  puffin  seagull\n",
       "0        False  False    True    False\n",
       "1        False  False   False     True\n",
       "2        False  False    True    False\n",
       "3        False   True   False    False\n",
       "4         True  False   False    False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's less performant, but you can also use pandas directly:\n",
    "pd.get_dummies(df['type']).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Polynomial Features\n",
    "\n",
    "Not everything can be linear, unfortunately. That said, sklearn has a [PolynomialFeatures mechanism](http://scikit-learn.org/stable/modules/preprocessing.html#generating-polynomial-features) which essentially maps polynomials to a matrix which can be mapped via linear regression. How does a linear regression fit a \"non-linear\" function? [Math](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html).\n",
    "\n",
    "* [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x236d2caaef0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmYFfWd7/H3t043Dci+Iy2b4IIKKq3iihoXcENFjUYjGmfMPpl7M1eTee69ZpKZeaLJTCaZSUiMGtEQUQGFKC64m1GUBlkUVFAMtCBrgyB006fqe/841d4Gm+6mu6ur+/Tn9Tw8fc6vqk59yyN8+lf1q1+ZuyMiIpKkIO0CREQk/ylsREQkcQobERFJnMJGREQSp7AREZHEKWxERCRxChsREUmcwkZERBKnsBERkcQVpF1Aa9GnTx8fOnRo2mWIiLQpixYt2uLufetbT2ETGzp0KKWlpWmXISLSppjZXxuynk6jiYhI4hQ2IiKSOIWNiIgkTmEjIiKJU9iIiEjiFDYiIpI4hY2IiCROYSMiIolT2IiISOIUNiIi7VC0+A3C0tdx9xbZn8JGRKSd8S0bCZ96DH93GaCwERGRZuZhluys6VBYSGbSdZi1TAwobERE2pHohafhk4/JXPZlrGu3FtuvwkZEpJ3wbVuIXn8JO3EcwVHHtui+9YgBEZF2wnr1IXPDrVjxkBbft3o2IiJ5zt3xLZsACIYfgXUoavEaFDYiInnOl5aS/c3PiNZ+mFoNChsRkTzm5VsJn3oMGzwUKx6aWh0KGxGRPOVRSDh7OpiRueIrWJDeP/kKGxGRPBW9PB8v+yuZiydj3XumWovCRkQkXxV2wE44heC4E9OuREOfRUTyVeaMc1ts7rP6qGcjIpJH3J3snBlEq1YCYGYpV5SjsBERySPRG6/iSxbi27akXco+FDYiInnCN5QRPfcEduQxBCefkXY5+1DYiIjkAd9bSXbmg9C5S26SzVZy+qyawkZEJA9ES0uhfCuZK6/HOh+SdjlfoNFoIiJ5ICg5jWDQYOzQw9IupVbq2YiItGFevhXfuhkza7VBAwobEZE2y8Ms4aMPkH3wd3gYpl1OnRQ2IiJtVPTcPHxDGZkJk7BMJu1y6qSwERFpg6J33yZa8HLuWs1Rx6VdTr0UNiIibYxv30Y4ZwY2sJjgwklpl9MgGo0mItLWdOlKMKaE4JQzsYK28c9426hSREQA8GwWKygkM+HytEs5KDqNJiLSRkQrlpKd+jN8+7a0SzloChsRkTbAt20hnPsI1qkzdO2WdjkHTWEjItLKebaK7KPTco93vuqrWKbtXQFR2IiItHLR03Pgk/VkrvgK1qNX2uU0SuJhY2YZM3vLzJ6I3w8zszfMbJWZPWxmHeL2ovj96nj50Bqf8cO4/T0zu7BG+4S4bbWZ/aBGe637EBFpazxbhW/aQHDaOQRHjEq7nEZriZ7N94CVNd7fCfzC3UcC5cAtcfstQLm7jwB+Ea+HmY0CrgWOASYAv4kDLAP8GpgIjAKui9etax8iIm2KFRSSmfItgnMnpl1KkyQaNmZWDFwM3BO/N+BcYGa8yjSgevzepPg98fIvxetPAma4e6W7rwFWAyfHf1a7+4fuvheYAUyqZx8iIm2CV+0lnDcb/2wXlsm0+ulo6pN0z+Y/gNuAKH7fG9ju7tn4fRkwKH49CFgHEC/fEa//eft+2xyova597MPMbjWzUjMr3bx5c2OPUUSkWbk74ZOziBa+hn/ycdrlNIvEwsbMLgE2ufuims21rOr1LGuu9i82ut/t7iXuXtK3b9/aVhERaXHRotfxpaUE488nOPzItMtpFkmOnzsduMzMLgI6At3I9XR6mFlB3PMoBtbH65cBhwFlZlYAdAe21WivVnOb2tq31LEPEZFWLSr7K9FTj2MjjyYYf37a5TSbxHo27v5Ddy9296HkLvC/4O7XAy8CV8WrTQHmxK/nxu+Jl7/g7h63XxuPVhsGjATeBBYCI+ORZx3ifcyNtznQPkREWi13J3pmDnTrnhvmbPlzd0oadwbdDswws38G3gLujdvvBR40s9XkejTXArj7O2b2CLACyALfdvcQwMy+AzwDZID73P2devYhItJqmRmZa78Gu3flZgrII5brCEhJSYmXlpamXYaItFPR+yuww49sc6POzGyRu5fUt17+9NFERNqoaOUywofuJXrzL2mXkhiFjYhIinzLJsLHZ2CDBhOcdHra5SRGYSMikhLfW0n2kfuhoIDM1Te2mQehNYbCRkQkJeG82bBlE5nJN2Dde6ZdTqLyN0ZFRFq5zLiz8MOGEQw/Iu1SEqewERFpYb7zU6xrN2zAIGxArbNp5R2dRhMRaUFevpXs1J8R/uWFtEtpUQobEZEW4nsryc74A7gTjBqddjktSmEjItIC3J1wzgzY/Enu0c69+qRdUotS2IiItIDoL8/jK5YRnHdx3szkfDAUNiIiLcB69yU4cRzBqWenXUoqNBpNRCRBHoZYJkMwagyMGpN2OalRz0ZEJCFesYfs3b8gWqpJfhU2IiIJ8CginD0dtmyEHr3SLid1ChsRkQRELz6Nr1pJMOEKgiHD0y4ndQobEZFmFi1bRPSX57ETxxGUnJp2Oa2CwkZEpJn5jnJs6AgyF12BmaVdTqug0WgiIs0sc+Z5+GnntLmnbiZJPRsRkWbgeyvJPvhborUfAiho9qOwERFpIvfcyDNf8wFUVaVdTquksBERaaLouXn4e+8QTJjULqeiaQiFjYhIE0RL3iR67UWCktMITjo97XJaLYWNiEgjuTvRB+9jw0cSTLhcI8/qoNFoIiKNZGZkrvwKVFVpQEA91LMRETlIXrGH7MwH8e3bMAuwDkVpl9TqKWxERA6Ch1nCR+7HVy7Dy7elXU6bobAREWkgdyd8Yia+ZjWZS68hGDYi7ZLaDIWNiEgDRa8+hy9ZSDD+AoLjT0q7nDZFYSMi0gCerSJasQwbPZZg/AVpl9PmaDSaiEgDWEEhBTd/GwoKNMS5EdSzERGpg2/ZSPbxh/CqvVhRRyyj39EbQ//VREQOwHftJDv9HqjaC5/t0hM3m0A9GxGRWnjVXsIZ98GunWSuuwVT0DSJwkZEZD8exbM4f7yOzOQbCAYNTrukNk9hIyKyv+3b8LVrcrM4H3Vs2tXkBV2zERHZj/XqQ8G3b8M6d0m7lLyhno2ISCxa/AbhC0/h7gqaZqawEREBovfeIXziUXz9OvAo7XLyTmJhY2YdzexNM1tqZu+Y2T/F7cPM7A0zW2VmD5tZh7i9KH6/Ol4+tMZn/TBuf8/MLqzRPiFuW21mP6jRXus+RERqE61bQzjzAWxgMZlrpmCBHhfQ3JLs2VQC57r7GOB4YIKZjQPuBH7h7iOBcuCWeP1bgHJ3HwH8Il4PMxsFXAscA0wAfmNmGTPLAL8GJgKjgOvidaljHyIi+/DNGwkfug+69eDTSV9l2cY9bN1VmXZZeSexsPGcXfHbwviPA+cCM+P2acDl8etJ8Xvi5V+y3JwQk4AZ7l7p7muA1cDJ8Z/V7v6hu+8FZgCT4m0OtA8RkX34pk+gsJAXT7yM0/7zTW645w1Ov/MF5i75OO3S8kqi12ziHsgSYBMwH/gA2O7u2XiVMmBQ/HoQsA4gXr4D6F2zfb9tDtTeu459iIgAuccFAATHjOHTm/4Hf/f0WiqqInZWZqmoirht1jL1cJpRomHj7qG7Hw8Uk+uJHF3bavHP2ma282Zs/wIzu9XMSs2sdPPmzbWtIiJ5yKuqCKffTbRiGQBlu0IKg33/OSwMAsrK96RRXl5qkdFo7r4deAkYB/Qws+r7e4qB9fHrMuAwgHh5d2Bbzfb9tjlQ+5Y69rF/XXe7e4m7l/Tt27cphygibYSHIeGj0/APVkEUAlDcsxNV0b4j0KqiiOKendIoMS8lORqtr5n1iF93As4DVgIvAlfFq00B5sSv58bviZe/4Ll+7lzg2ni02jBgJPAmsBAYGY8860BuEMHceJsD7UNE2jH3iHDOQ/iqlQQXX0lw7AkA9O5SxF2TR9OxMKBrUQEdCwPumjya3l2KUq44fyQ5g8BAYFo8aiwAHnH3J8xsBTDDzP4ZeAu4N17/XuBBM1tNrkdzLYC7v2NmjwArgCzwbXcPAczsO8AzQAa4z93fiT/r9gPsQ0TaKXcnmvcYvvwtgnMvIlNy2j7LLzt+EKeP6ENZ+R6Ke3ZS0DQzq75IVudKZpnqf+DzVUlJiZeWlqZdhogkxN2Jnp0LQUBw3iV6AFozMbNF7l5S33oN7dmsNrOZwB/cfUXTShMRaVleWYEVdSS44DIABU0KGnrNZjTwPnCPmS2IR3F1S7AuEZFmES1eQPa/7sTLt2JmCpqUNChs3H2nu//e3U8DbgPuADaY2TQzG5FohSIijRS9s5TwzzOxAYdCt+5pl9OuNShs4pszLzOzx4BfAv8GDAf+DMxLsD4RkUaJVr9LOHs6Nnhobr6zjJ6okqaG/tdfRW448c/c/bUa7TPN7KzmL0tEpPGij9cSPvwH6Nc/90jnQs3Fm7Z6wyYeuny/u/+4tuXu/nfNXpWISBNY3/4Eo0sIzp2IddSNma1BvafR4iHP57RALSIiTeIb1+dGnnUoInPp1dghegBaa9HQ0Wivmdl/mdmZZnZi9Z9EKxMROQi+oYzs/b8hfHJm/StLi2voNZvqW21rnkqrflyAiEiqfOMGsg/+Doo6kjn3orTLkVo0KGzcXafRRKRV8i0byT74WygooODGb2A9eqVdktSiwWMBzexick/L7FjddqBBAyIiLcHdCR97CDAKbvwm1qtP2iXJATQobMzst0BncgMF7iE3o/KbCdYlIlIvMyMz+QbIZrE+/dIuR+rQ0AECp7n7jUC5u/8TcCr7PktGRKTFePlWwpefxd2xXn2wfgPSLknq0dCwqX5c3W4zOxSoAoYlU5KIyIF5+Vay06YSLXgFdpSnXY40UEOv2TwRPwjtZ8BiciPR7kmsKhGRWvi2LWSnTYWqvRoM0MY0dDTaT+KXs8zsCaCju+9IriwRkX35ti1k7/8NZKtyQTNgUNolyUGoM2zM7Mo6luHus5u/JBGRL/KtmwGnYMo3sf6Hpl2OHKT6ejaX1rHMAYWNiCTKq/ZihR0IRh6NffcfscLCtEuSRqgzbNz95pYqRERkf755I9kHf0dmwiSCUWMUNG2YbuoUkVbJN31C9oHfArlZnKVt002dItLqROvXEf7xbsgUUDDlG1gfhU1bp5s6RaRV8R3lhNOmQlFHCr72HQVNnmjoabSK+Gf1TZ3b0E2dIpKEbj0Izr6Q4JgxWLceaVcjzaShYfPnWm7q/H1iVYlIuxO99zbWoxfW/1Ayp45PuxxpZg09jfYuELr7LODXwALg8cSqEpF2JVq+mPDhaYQvPJV2KZKQhobN/3H3nWZ2BnA+cD8wNbGqRKTdiBYtIJz9J2zwMDJXXp92OZKQhoZNGP+8GPitu88BOiRTkoi0F+HrLxM+8Sg24kgy1/8NVtSx/o2kTWpo2HxsZr8DrgHmmVnRQWwrIvIFHkX4B+9iR48mc+3NWKF+f81nDR0gcA0wAfi5u283s4HA/0quLBHJVx6FsHcv1rETmS/fDJkCLNDvrvmuobM+76bGPGjuvgHYkFRRIpKfvKqKcPYf4dMdZL72HfVm2hH9OiEiLcIr9hBOvxt/9x1s9Fgs0+DZsiQP6NsWkcT5zk/JTr8bNm8iM/l6gmNPSLskaWEKGxFJXDjnIdi2lcxXbiE4/Mi0y5EUKGxEJHGZi6/Cd39GMGhw2qVISnTNRkRqtXVXJUvXbWfrrspGbR+9v4LwiZm4O9azt4KmnVPPRkS+YM6Sj7l91jIKg4CqKOKuyaO57PhBDd4+XPga0VOzYcAggsoK6NgpwWqlLVDPRkT2sXVXJbfPWkZFVcTOyiwVVRG3zVrWoB6Oe0Q4/89E82ZhI46i4KZvYQoaQT0bEdlPWfkeCoOACqLP2wqDgLLyPfTuUlTntuETs/DFCwhKTiOYeDkWZJIuV9qIxHo2ZnaYmb1oZivN7B0z+17c3svM5pvZqvhnz7jdzOxXZrbazJaZ2Yk1PmtKvP4qM5tSo32smS2Pt/mVmVld+xCR+hX37ERVFO3TVhVFFPesv4cSHDOG4PxLCC66UkEj+0jyNFoW+L67Hw2MA75tZqOAHwDPu/tI4Pn4PcBEYGT851biWaXNrBdwB3AKcDJwR43wmBqvW73dhLj9QPsQkXr07lLEXZNH07EwoGtRAR0LA+6aPPqAvRov30r01hsABMOPIHPaOcS/94l8LrHTaDWntIkfT7ASGARMAs6OV5sGvATcHrc/4O4OLDCzHvEcbGcD8919G4CZzQcmmNlLQDd3fz1ufwC4HHiqjn2ISANcdvwgTh/Rh7LyPRT37HTAoInWrSGccT/g2FHHYZ06t2id0na0yDUbMxsKnAC8AfSPgwh332Bm/eLVBgHramxWFrfV1V5WSzt17ENEGqh3l6I6r9FESxYSPvEodO9JwVf+RkEjdUo8bMysCzAL+Ht3/7SO7nVtC7wR7QdT263kTsMxeLDuARBpqPD5eUR/eR4bNpLM1TcqaKReiQ59NrNCckEz3d2rZ43eGJ8eI/65KW4vAw6rsXkxsL6e9uJa2uvaxz7c/W53L3H3kr59+zbuIEXaoy5dCUpOI3P93ypopEGSHI1mwL3ASnf/9xqL5gLVI8qmAHNqtN8Yj0obB+yIT4U9A1xgZj3jgQEXAM/Ey3aa2bh4Xzfu91m17UNEGsm3byNaswqA4OQzciPOMhpxJg2T5Gm004GvAsvNbEnc9o/AT4FHzOwWYC1wdbxsHnARsBrYDdwM4O7bzOwnwMJ4vR9XDxYAvgncD3QiNzDgqbj9QPsQkUaI1n5I+PA0KCjAvvtDrEC36MnBsdzgLykpKfHS0tK0yxBpdaK33iB8clZuIMB1t2B9NN5G/j8zW+TuJfWtp19PRKRWHkVE82YTLXodGz6SzFUaCCCNp7ARkdqZ4VFEcNo5BF+aqBkBpEkUNiKyj2jdGqyoE9ZvAJlLr8JM8/VK0+n/IhEBwN0JS18jvH8q4fw/AyhopNmoZyMieLaK8MnZ+JI3sRFHkbny+rRLkjyjsBFp53zXTsKH7sXXryM48zyCsy/EAvVopHkpbETau06dofMhZL58E8FRx6VdjeQp/foi0g55GBK+/Cy++zMsk6Hg+r9V0Eii1LMRaWd8RznhzAfxsr9iXbpiY09NuyRpBxQ2Iu1ItPpdwtnTIQzJTL6B4NgT0i5J2gmFjUg7ES0tJXz8Ieg3kIJrpmC9NdO5tByFjUg7YSOOJDh1PME5E7HCwrTLkXZGAwRE8li0fDHZP92DRyF2SFcyF1ymoJFUqGcjkoe8soLwqcfwpaXYYUOhYg907pJ2WdKOKWxE8kz08VrCWX+E7dsIxl9AcNZ5mkRTUqewEckjHkW5QQBRROambxEMHp52SSKAwkYkL3j5VujSFSvsQME1N+Ve69kz0opogIBIG1Y9U3N26s+JXnoGAOvbX0EjrY56NiJtlH+6nXDuw/gH72PDjyA4+Yy0SxI5IIWNSBsUrX6XcOaDEEUEF00mKDkVM0u7LJEDUtiItEHWszc2aDCZiydjvfqkXY5IvXTNRqQNcHeiZYvIzn0EAOvdl4Kvfl1BI22GejYirZxv30b45Cx89btY8RC8sgIr6ph2WSIHRWEj0kp5FBEt/G+i5+cBEEy4nOCk0/UUTWmTFDYirVVlBdErz2FDhueuzfTolXZFIo2msBFpRTxbRfTWmwRjT8U6dabgb78H3XtqpJm0eQobkVYiWrWS8KnHoHwr1r0ndsQo9WYkbyhsRFLmO8oJn5mDr1wOvfuS+erXCYYfkXZZIs1KYSOSIncnfGQavukTgnMnEpx6Nlagv5aSf/R/tUgKoo8+wAYOwoo6krnkKujUWafMJK8pbERakJdvJZz/Z3zlcoLxF5A5+0JsYHHaZYkkTmEj0gK8soLo1eeIFrwCQYbgnAkEp56ddlkiLUZhI9ICwidn4csXY2NKyHzpIqxr97RLEmlRChuRhEQfrcZ69MJ69CIz/gL8lDMJBg1OuyyRVChsRJqZf/Ix4fPz8NXvEow9lcwlV2G9+6LbMqU9U9iINBMv30r44lP48regYyeC8y7RA81EYgobkWYSLXgVX/k2wennEpxxLtaxU9olibQaChuRRvI9u4kWvJJ7JPOQ4QTjzyM44xxd/BephcJG5CBVh0z0xqtQWUFgBkOGY527pF2aSKuV2IMxzOw+M9tkZm/XaOtlZvPNbFX8s2fcbmb2KzNbbWbLzOzEGttMiddfZWZTarSPNbPl8Ta/snha3APtQ6Q5hAteIfvLfyF6ZT52+BEUfOP7ZM6+MO2yRFq9JJ/CdD8wYb+2HwDPu/tI4Pn4PcBEYGT851ZgKuSCA7gDOAU4GbijRnhMjdet3m5CPfsQaRTfsxuPwviNxyHzDxRcPQXrf2i6xYm0EYmFjbu/Amzbr3kSMC1+PQ24vEb7A56zAOhhZgOBC4H57r7N3cuB+cCEeFk3d3/d3R14YL/Pqm0fIgelejbm7C9+khthBgTjzopDZmDK1Ym0LS19zaa/u28AcPcNZtYvbh8ErKuxXlncVld7WS3tde1DpEF80wbC117Cly8GBzvuBOzQ3PxleoiZSOO0lgECtf0N9ka0H9xOzW4ldyqOwYN1Z7fkpvzPzp4O27YSnHQ6wbizNBuzSDNo6bDZaGYD4x7HQGBT3F4GHFZjvWJgfdx+9n7tL8XtxbWsX9c+vsDd7wbuBigpKTnosJK2z7NZfMVSoiULyXz5JqyoIwVXfAW6dsc6H5J2eSJ5I8kBArWZC1SPKJsCzKnRfmM8Km0csCM+FfYMcIGZ9YwHBlwAPBMv22lm4+JRaDfu91m17UPkc77zU8IXnyb7H/9M+Nif8E+3Q/lWAKz/oQoakWaWWM/GzB4i1yvpY2Zl5EaV/RR4xMxuAdYCV8erzwMuAlYDu4GbAdx9m5n9BFgYr/djd68edPBNciPeOgFPxX+oYx8iAPin28n+8l8hirCRRxGcciY2fCRmLf27l0j7YbnBXFJSUuKlpaVplyEJ8Io9RMsWwa6dZM6dCED4xqsEI4/GevVJuTqRts3MFrl7SX3rtZYBAiLNyt3xtWuIFi/AVyyFbBYrHoJHF2JBQOaUM9MuUaRdUdhIXopee5HouSehqCPBmJMIxo5r0uOXt+6qpKx8D8U9O9G7S1EzVirSPihspM3zygp85TKi5YsJTjmL4IhRBMccjx3SFRs1GuvQtHCYs+Rjbp+1jMIgoCqKuGvyaC47flD9G4rI5xQ20iZ5FOGrVhItX4y/9zZks9CzN2SrAHJPyDy+6ffHbN1Vye2zllFRFVFBBMBts5Zx+og+6uGIHASFjbQZnq2CrZtz85EZhE89BlV7CU44BRs9Fhs0uNnv8C8r30NhEHweNACFQUBZ+R6FjchBUNhIq+Z7K/FV7xKtXIavWgkFhRR8/w4sCCj46tehRy8sk0ls/8U9O1EVRfu0VUURxT31YDSRg6GwkVYrLH2d6JnHc6fIOnfBjj2B4OjjPl9uvfsmXkPvLkXcNXk0t+13zUa9GpGDo7CRg9bcI7M8zOJr1+R6MKtXkrn0aoLDhmH9+hOcOA47ejQ2eBgWpHPT5WXHD+L0EX00Gk2kCRQ2clCac2SW79xBOPcRfO0a2FsJmQw25PDPp1QNBg+HwcObsfrG692lSCEj0gQKG2mwxo7M8iiEjRuI/voh/tFqbOBhZMafD50PwXftJBg9FhtxJDZsZJOHKYtI66SwaaL2dLNfQ0dm+d7Kz0Mj++g0fNW7ULU3t7Bnb2zQEAAsU0Dh1/9nyx2AiKRGYdME7e1mv9pGZvWMdjN060eEH27GN67HN5RBEFD43R8C8f0uJ5yCFQ/OXXfp3rO2jxaRPKewaaT2dLOfu8POHfTcupkZJzqLlqzmlxzJ3sj5w+E76Dznwdx/gV59sEMPy81B5o6ZkTn/0rTLF5FWQGHTSDVPKf26wzIKcDYGh7D7VaPn0EFYvwEtMjS3uXhlBezYnnuuy45ybNQYrFNnokWvEz4z9/PTYMcAozoUMm7yFQwo7k+v3dvwyvFYv4FYUcd0D0JEWi2FTSPVPKVU4RlGBp9xMtvp8uZawjfBjjuRgiuvB6Bq6s+xoiLo0g3r0hW6dMWGHE4wZDjuEf7xutw/1B07QlFHKOzQ6Dvh3SOoqoIggxUU5OYN+2Q97N4Fuz/DP/sMdu/KTUzZdwDR+ysIZ0+Hyop9PifTdwA2eBj06U9w4inQux/Wuw/Wpx907cYx1c9+6TKw1md0i4jUpLBppJo3+/0oGJO7ZnPlcVx6VK/cEx8LCgHwMMT6DYBdO/HNG/E1q6BiD8Hp58CQ4VBRQXjvr/b7dCM4dyKZM7+Ue9DX738JmQwUxF+XO5mzzicYU4Jv3kj2gakQea73EfdAMpdfh40pwTeuJ7z/1/t+fIcibPgRWN8BWI9eBKPHQvceuesp3Xpg3XtA1+4ABEOG5+oUEWkCPTwt1tiHpzVmNJpnsxCFWIciPJv9PIC8sgIqKmBvJTZ8JMHQEfhnuwifnwdRmLuTHiAwgtElBCOOwj/dQfjys7kbHgsLoUMRdOhAMOJorN8AvGIP/vHa3GOOD+kCnQ/B4iAUEWmqhj48TWET05M6RUQOXkPDRg9dFxGRxClsREQkcQobERFJnMJGREQSp7AREZHEKWxERCRxChsREUmcwkZERBKnmzpjZrYZ+GsjN+8DbGnGctoCHXP7oGPOf0093iHuXu+swwqbZmBmpQ25gzaf6JjbBx1z/mup49VpNBERSZzCRkREEqewaR53p11ACnTM7YOOOf+1yPHqmo2IiCROPRsREUmcwqaJzGyCmb1nZqvN7Adp19MSzOwjM1tuZkvMLC8fAmRm95nZJjN7u0ZbLzObb2ar4p8906yxOR3geH9kZh/H3/MSM7sozRqbm5kdZmYvmtlKM3vHzL4Xt+fz93ygY078u9ZptCYwswzwPnA+UAYsBK5z9xWpFpYwM/sIKHH3vL0XwczOAnYBD7j7sXHbXcBs7AKzAAAEgElEQVQ2d/9p/ItFT3e/Pc06m8sBjvdHwC53/3matSXFzAYCA919sZl1BRYBlwM3kb/f84GO+RoS/q7Vs2mak4HV7v6hu+8FZgCTUq5JmoG7vwJs2695EjAtfj2N3F/SvHCA481r7r7B3RfHr3cCK4FB5Pf3fKBjTpzCpmkGAetqvC+jhb64lDnwrJktMrNb0y6mBfV39w2Q+0sL9Eu5npbwHTNbFp9my5vTSfszs6HACcAbtJPveb9jhoS/a4VN01gtbe3hvOTp7n4iMBH4dnwKRvLPVOBw4HhgA/Bv6ZaTDDPrAswC/t7dP027npZQyzEn/l0rbJqmDDisxvtiYH1KtbQYd18f/9wEPEbudGJ7sDE+51197ntTyvUkyt03unvo7hHwe/LwezazQnL/6E5399lxc15/z7Udc0t81wqbplkIjDSzYWbWAbgWmJtyTYkys0PiC4uY2SHABcDbdW+VN+YCU+LXU4A5KdaSuOp/cGNXkGffs5kZcC+w0t3/vcaivP2eD3TMLfFdazRaE8VDBP8DyAD3ufu/pFxSosxsOLneDEAB8Kd8PGYzewg4m9yMuBuBO4DHgUeAwcBa4Gp3z4uL6gc43rPJnVZx4CPg69XXMvKBmZ0BvAosB6K4+R/JXcPI1+/5QMd8HQl/1wobERFJnE6jiYhI4hQ2IiKSOIWNiIgkTmEjIiKJU9iIiEjiFDYibYCZfcPMbky7DpHG0tBnkVbOzArcPZt2HSJNoZ6NSCOZ2Q1m9mb8/I/fmdmQ+BkofcwsMLNXzewCMxtqZu+a2bR4osOZZtY5/oyxZvZyPKnpMzWmSXnJzP7VzF4Gvhc/b+Qf4mWHm9nT8TavmtlRcfv9ZvYrM3vNzD40s6tq1Hqb5Z5BtNTMflrX54gkQWEj0ghmdjTwZXKTkh4PhMB44E7gt8D3gRXu/my8yZHA3e4+GvgU+FY8R9V/Ale5+1jgPqDmbAw93H28u+8/KeLdwHfjbf4B+E2NZQOBM4BLgOpQmUhumvxT3H0McFcDPkekWRWkXYBIG/UlYCywMDfdFJ2ATe7+IzO7GvgGuek/qq1z9/+OX/8R+DvgaeBYYH78GRlyM+5We3j/ncaz9Z4GPBpvA1BUY5XH48kUV5hZ/7jtPOAP7r4bwN23NeBzRJqVwkakcQyY5u4/3Kcxd3qsOH7bBdgZv97/4qjHn/GOu596gH18VktbAGyPe1O1qdyvxuqf+++/vs8RaVY6jSbSOM8DV5lZP/j8ufVDyJ1Gmw78X3JTtVcbbGbVoXId8BfgPaBvdbuZFZrZMXXtNH72yJq494TljKmn1meBr9W4TtSrkZ8j0mgKG5FGcPcVwP8m98TSZcB8YChwEnCnu08H9prZzfEmK4Ep8bq9gKnxo8SvAu40s6XAEnKntupzPXBLvM071PMocnd/mty0+aVmtoTc9ZmD/hyRptDQZ5GExY/ffcLdj025FJHUqGcjIiKJU89GREQSp56NiIgkTmEjIiKJU9iIiEjiFDYiIpI4hY2IiCROYSMiIon7f4tovFyRYbVaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print and show dataset\n",
    "df = pd.read_csv('data/othello.csv', index_col=0)\n",
    "ax = df.plot.scatter(x='experience', y='salary')\n",
    "\n",
    "# Don't worry about this. We'll get to this later.\n",
    "pipe = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('line', LinearRegression(fit_intercept=True))\n",
    "])\n",
    "\n",
    "# This is a pipe, so we only need fit() and predict().\n",
    "pipe.fit(\n",
    "    df['experience'].values.reshape(-1,1),\n",
    "    df['salary'].values.reshape(-1,1)\n",
    ")\n",
    "\n",
    "# Linspace is awesome.\n",
    "linx = np.linspace(0,25, 1000).reshape(-1,1)\n",
    "\n",
    "ax.plot(linx, pipe.predict(linx), color='salmon', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 2., 4., 8.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How does that work?\n",
    "pf = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Define data.\n",
    "data = np.array([\n",
    "    [0], \n",
    "    [1], \n",
    "    [2]\n",
    "])\n",
    "\n",
    "# Fit transform data which can then be mapped by complex linear regressions.\n",
    "pf.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other transformations\n",
    "\n",
    "Sklearn also gives you the flexibility to put arbitrary functions in a given pipeline using a [function transformer](http://scikit-learn.org/stable/modules/preprocessing.html#function-transformer). This means that you can supply your own functions such as logs or inverse logs to arbitrarily transform arrays. Logarithmic transformations are useful for features that do not scale linearly. \n",
    "\n",
    "* [FunctionTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.30103   ],\n",
       "       [0.47712125],\n",
       "       [0.60205999],\n",
       "       [0.69897   ],\n",
       "       [0.77815125],\n",
       "       [0.84509804],\n",
       "       [0.90308999],\n",
       "       [0.95424251],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log transformer\n",
    "function_transformer = FunctionTransformer(np.log10)\n",
    "\n",
    "# Create data\n",
    "X = np.arange(1,11,1).reshape(-1,1)\n",
    "\n",
    "# Map data.\n",
    "function_transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "More features isn't necessarily better (see [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)). You generally want to eliminate less features, which can be done via statistical tests, model usage, or recursive feature selection (RFE).\n",
    "\n",
    "* [SelectFromModel](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel)\n",
    "* [RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)\n",
    "* [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)\n",
    "\n",
    "Lets select our \"best\" features with each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "X = df.iloc[:,[0,1,2,3]]\n",
    "y = df.iloc[:,[4]].values.ravel()\n",
    "\n",
    "# Create and fit base model for RFE and SelectFromModel\n",
    "dtree = RandomForestClassifier()\n",
    "dtree.fit(X ,y)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Via SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree says our best features are ['petal_length', 'petal_width'].\n"
     ]
    }
   ],
   "source": [
    "# Select features by wrapping in SelectFromModel.\n",
    "model = SelectFromModel(dtree, prefit=True)\n",
    "new = model.transform(X)\n",
    "\n",
    "# Get best features out.\n",
    "best_features = X.columns[model.get_support()]\n",
    "bf_as_list = best_features.values.tolist()\n",
    "print('Decision tree says our best features are {}.'.format(bf_as_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Via RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination says our best ranking is: ['sepal_width', 'petal_width', 'petal_length', 'sepal_length'] if selecting only one feature.\n"
     ]
    }
   ],
   "source": [
    "# Select features by wrapping in REFE \n",
    "model = RFE(estimator=dtree, n_features_to_select=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get best features out.\n",
    "ranking = model.ranking_\n",
    "rank_names = [X.columns[rank - 1] for rank in ranking]\n",
    "print('Recursive feature elimination says our best ranking is: {} if selecting only one feature.'.format(rank_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi squared says our best features are ['petal_length', 'petal_width'].\n"
     ]
    }
   ],
   "source": [
    "### Via Kbest (requires no previous model, as it's a statistical test)\n",
    "model = SelectKBest(chi2, k=2)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get best features out\n",
    "best_features = X.columns[model.get_support()]\n",
    "bf_as_list = best_features.values.tolist()\n",
    "print('Chi squared says our best features are {}.'.format(bf_as_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) and [Linear Discriminant Analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) ([also](http://scikit-learn.org/stable/modules/lda_qda.html#lda-qda)) are often used to find useful combinations of dimensions.\n",
    "\n",
    "* [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n",
    "* [LinearDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our explained variance ratios are [0.92461872 0.05306648 0.01710261 0.00521218].\n"
     ]
    }
   ],
   "source": [
    "# Load IRIS\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "X = df.iloc[:,[0,1,2,3]]\n",
    "y = df.iloc[:, [4]].values.ravel()\n",
    "\n",
    "# Run PCA\n",
    "pca = PCA(n_components=4)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "# Describe\n",
    "print('Our explained variance ratios are {}.'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction\n",
    "\n",
    "A lot of times you will want to turn large bodies of [text into numbers](https://en.wikipedia.org/wiki/Text_mining). You can do this with sparse matrices and [tf-idf transformation](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). This will allow you to feed text data to your ML algorithms in a sane manner. See also the Natural Language Toolkit ([NLTK](https://en.wikipedia.org/wiki/Natural_Language_Toolkit))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "['Mortgage' 'Debt collection'\n",
      " 'Credit reporting, credit repair services, or other personal consumer reports']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank account or service</th>\n",
       "      <th>Checking or savings account</th>\n",
       "      <th>Consumer Loan</th>\n",
       "      <th>Credit card</th>\n",
       "      <th>Credit card or prepaid card</th>\n",
       "      <th>Credit reporting</th>\n",
       "      <th>Credit reporting, credit repair services, or other personal consumer reports</th>\n",
       "      <th>Debt collection</th>\n",
       "      <th>Money transfer, virtual currency, or money service</th>\n",
       "      <th>Money transfers</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Other financial service</th>\n",
       "      <th>Payday loan</th>\n",
       "      <th>Payday loan, title loan, or personal loan</th>\n",
       "      <th>Prepaid card</th>\n",
       "      <th>Student loan</th>\n",
       "      <th>Vehicle loan or lease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The payday loan store charges too much interest and rolled me into a new loan.</th>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.007190</td>\n",
       "      <td>0.414327</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>0.010056</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.021064</td>\n",
       "      <td>0.024376</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>0.006893</td>\n",
       "      <td>0.057930</td>\n",
       "      <td>0.007847</td>\n",
       "      <td>0.084004</td>\n",
       "      <td>0.238057</td>\n",
       "      <td>0.011181</td>\n",
       "      <td>0.053473</td>\n",
       "      <td>0.013656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Debt collectors are calling me 24 hours a day. I do not have the money.</th>\n",
       "      <td>0.032181</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.039808</td>\n",
       "      <td>0.013207</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>0.016404</td>\n",
       "      <td>0.686461</td>\n",
       "      <td>0.102364</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>0.011689</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.003257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh no how did i get here i am not good with computer</th>\n",
       "      <td>0.035816</td>\n",
       "      <td>0.029114</td>\n",
       "      <td>0.077976</td>\n",
       "      <td>0.055404</td>\n",
       "      <td>0.038129</td>\n",
       "      <td>0.098114</td>\n",
       "      <td>0.196750</td>\n",
       "      <td>0.263565</td>\n",
       "      <td>0.016820</td>\n",
       "      <td>0.009203</td>\n",
       "      <td>0.076486</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.035421</td>\n",
       "      <td>0.017847</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>0.010024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Bank account or service  \\\n",
       "The payday loan store charges too much interest...                 0.011786   \n",
       "Debt collectors are calling me 24 hours a day. ...                 0.032181   \n",
       "oh no how did i get here i am not good with com...                 0.035816   \n",
       "\n",
       "                                                    Checking or savings account  \\\n",
       "The payday loan store charges too much interest...                     0.007190   \n",
       "Debt collectors are calling me 24 hours a day. ...                     0.010906   \n",
       "oh no how did i get here i am not good with com...                     0.029114   \n",
       "\n",
       "                                                    Consumer Loan  \\\n",
       "The payday loan store charges too much interest...       0.414327   \n",
       "Debt collectors are calling me 24 hours a day. ...       0.016036   \n",
       "oh no how did i get here i am not good with com...       0.077976   \n",
       "\n",
       "                                                    Credit card  \\\n",
       "The payday loan store charges too much interest...     0.012564   \n",
       "Debt collectors are calling me 24 hours a day. ...     0.039808   \n",
       "oh no how did i get here i am not good with com...     0.055404   \n",
       "\n",
       "                                                    Credit card or prepaid card  \\\n",
       "The payday loan store charges too much interest...                     0.010056   \n",
       "Debt collectors are calling me 24 hours a day. ...                     0.013207   \n",
       "oh no how did i get here i am not good with com...                     0.038129   \n",
       "\n",
       "                                                    Credit reporting  \\\n",
       "The payday loan store charges too much interest...          0.011140   \n",
       "Debt collectors are calling me 24 hours a day. ...          0.008226   \n",
       "oh no how did i get here i am not good with com...          0.098114   \n",
       "\n",
       "                                                    Credit reporting, credit repair services, or other personal consumer reports  \\\n",
       "The payday loan store charges too much interest...                                           0.021064                              \n",
       "Debt collectors are calling me 24 hours a day. ...                                           0.016404                              \n",
       "oh no how did i get here i am not good with com...                                           0.196750                              \n",
       "\n",
       "                                                    Debt collection  \\\n",
       "The payday loan store charges too much interest...         0.024376   \n",
       "Debt collectors are calling me 24 hours a day. ...         0.686461   \n",
       "oh no how did i get here i am not good with com...         0.263565   \n",
       "\n",
       "                                                    Money transfer, virtual currency, or money service  \\\n",
       "The payday loan store charges too much interest...                                           0.014455    \n",
       "Debt collectors are calling me 24 hours a day. ...                                           0.102364    \n",
       "oh no how did i get here i am not good with com...                                           0.016820    \n",
       "\n",
       "                                                    Money transfers  Mortgage  \\\n",
       "The payday loan store charges too much interest...         0.006893  0.057930   \n",
       "Debt collectors are calling me 24 hours a day. ...         0.009961  0.010921   \n",
       "oh no how did i get here i am not good with com...         0.009203  0.076486   \n",
       "\n",
       "                                                    Other financial service  \\\n",
       "The payday loan store charges too much interest...                 0.007847   \n",
       "Debt collectors are calling me 24 hours a day. ...                 0.003574   \n",
       "oh no how did i get here i am not good with com...                 0.013630   \n",
       "\n",
       "                                                    Payday loan  \\\n",
       "The payday loan store charges too much interest...     0.084004   \n",
       "Debt collectors are calling me 24 hours a day. ...     0.010606   \n",
       "oh no how did i get here i am not good with com...     0.005329   \n",
       "\n",
       "                                                    Payday loan, title loan, or personal loan  \\\n",
       "The payday loan store charges too much interest...                                   0.238057   \n",
       "Debt collectors are calling me 24 hours a day. ...                                   0.011689   \n",
       "oh no how did i get here i am not good with com...                                   0.035421   \n",
       "\n",
       "                                                    Prepaid card  \\\n",
       "The payday loan store charges too much interest...      0.011181   \n",
       "Debt collectors are calling me 24 hours a day. ...      0.009202   \n",
       "oh no how did i get here i am not good with com...      0.017847   \n",
       "\n",
       "                                                    Student loan  \\\n",
       "The payday loan store charges too much interest...      0.053473   \n",
       "Debt collectors are calling me 24 hours a day. ...      0.015197   \n",
       "oh no how did i get here i am not good with com...      0.020373   \n",
       "\n",
       "                                                    Vehicle loan or lease  \n",
       "The payday loan store charges too much interest...               0.013656  \n",
       "Debt collectors are calling me 24 hours a day. ...               0.003257  \n",
       "oh no how did i get here i am not good with com...               0.010024  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get CFPB data\n",
    "df = pd.read_csv('data/cfpb.csv', nrows=5000, encoding='cp1252')\n",
    "\n",
    "# Clean up. Do as I say not as I do. Always train test split.\n",
    "dataset = df.dropna(subset=['Consumer complaint narrative'])\n",
    "series = dataset['Consumer complaint narrative']\n",
    "\n",
    "# Create vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Create model.\n",
    "svc = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Vectorize.\n",
    "X = vectorizer.fit_transform(series.values)\n",
    "y = dataset['Product']\n",
    "\n",
    "# Fit model.\n",
    "svc.fit(X, y)\n",
    "\n",
    "# Dummy complaints\n",
    "complaints = [\n",
    "    'The payday loan store charges too much interest and rolled me into a new loan.',\n",
    "    'Debt collectors are calling me 24 hours a day. I do not have the money.',\n",
    "    'oh no how did i get here i am not good with computer'\n",
    "]\n",
    "\n",
    "# Transform\n",
    "complaints_processed = vectorizer.transform(complaints)\n",
    "\n",
    "# Predict classes.\n",
    "print('Predictions:')\n",
    "print(svc.predict(complaints_processed))\n",
    "\n",
    "# Predict proba\n",
    "probability = svc.predict_proba(complaints_processed)\n",
    "categories  = svc.classes_\n",
    "pd.DataFrame(\n",
    "    data=probability,\n",
    "    columns=categories,\n",
    "    index=complaints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Learing Resources\n",
    "\n",
    "* ### [Sklearn Preprocessing Data](http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "* ### [Sklearn Feature Selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)\n",
    "* ### [Sklearn Text Analysis](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "* ### [Sklearn Model Selection and Evaluation](http://scikit-learn.org/stable/model_selection.html)\n",
    "* ### [Sklearn Dataset Transformations](http://scikit-learn.org/stable/data_transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Up: [Modeling](4_modeling.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"margin-left: 0;\" src=\"static/svm_hyperplanes.svg\" width=\"20%\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg'>ZackWeinberg</a> under the <a href='https://creativecommons.org/licenses/by-sa/3.0/deed.en'>CC BY-SA 3.0</a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
