{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://github.com/theonaunheim\">\n",
    "    <img style=\"border-radius: 100%; float: right;\" src=\"static/strawberry_thief_square.png\" width=10% alt=\"Theo Naunheim's Github\">\n",
    "</a>\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "<h1 align='center'>Modeling</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: table; width: 100%\">\n",
    "    <div style=\"display: table-row; width: 100%;\">\n",
    "        <div style=\"display: table-cell; width: 50%; vertical-align: middle;\">\n",
    "            <img src=\"static/svm_hyperplanes.svg\" width=\"300\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 10%\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 40%; vertical-align: top;\">\n",
    "            <blockquote>\n",
    "                <p style=\"font-style: italic;\">\"Essentially, all models are wrong, but some are useful\"</p>\n",
    "                <br>\n",
    "                <p>- George E. P. Box</p>\n",
    "            </blockquote>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg'>ZackWeinberg</a> under the <a href='https://creativecommons.org/licenses/by-sa/3.0/deed.en'>CC BY-SA 3.0</a>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generally\n",
    "\n",
    "Modeling is generally the stage most people think of when describing ML. Conceptually what we are doing is we are translating a complex process to a simplified model of the process which we can better use and manipulate. In the case of ML, we are taking observed data and feeding it to a ML algorithm. The algorithm fits our observed data and gives us a model to use.\n",
    "\n",
    "Reposting for reference:\n",
    "<img src='static/supervised_ml_flowchart_annotated.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Prep our data\n",
    "\n",
    "Usually we will want to load and process our data so that our algorithm can work well. To reiterate, we use **X for our feature matrix** (the data we will use to predict), and **y for our target vector** (the data will try to predict). In the example below, we limit the amount of input data to 10,000 rows for the sake of time. Do not do this. More data will generally give you better models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data. \n",
    "rdf = pd.read_csv('data/diamonds.csv', nrows=10000)\n",
    "\n",
    "# Encode cut category info as binary\n",
    "cut_df = pd.get_dummies(rdf['cut'])\n",
    "df = pd.concat([rdf, cut_df], axis=1)\n",
    "\n",
    "# Map colors to rank (Best D -> Worst Z)\n",
    "le = LabelEncoder()\n",
    "le.fit(sorted(df['color'].unique()))\n",
    "df['color_rank'] = le.transform(df['color'])\n",
    "\n",
    "# Get our features\n",
    "X = df[['carat', 'color_rank', 'depth', 'x', 'y',\n",
    "        'x', 'Ideal', 'Premium', 'Very Good', 'Good',\n",
    "        'Fair',\n",
    "]]\n",
    "\n",
    "# And our targets\n",
    "y = df[['price']]\n",
    "\n",
    "# Scale X and y.\n",
    "x_scaler = StandardScaler()\n",
    "X_scaled = x_scaler.fit_transform(X)\n",
    "y_scaler = StandardScaler()\n",
    "y_scaled = y_scaler.fit_transform(y).ravel()\n",
    "\n",
    "# Train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n",
    "\n",
    "# Show unscaled.\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create ML Algorithm\n",
    "\n",
    "Our first step is to create our algorithm and feed the necessary arguments or [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) for the algorithm to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(\n",
    "    n_estimators=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Feed algorithm feature data and target data via the .fit() method.\n",
    "\n",
    "Depending on how computationally heavy your algorithm is, this could take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_scaled, y_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Predict using our newly fitted classifer.\n",
    "\n",
    "You can use predict() or predict_proba() depending on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = clf.predict(X_test)\n",
    "y_actual    = y_test\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'z_score_prediction': y_predicted,\n",
    "    'z_score_actual'    : y_actual,\n",
    "    'price_prediction'  : y_scaler.inverse_transform(y_predicted),\n",
    "    'price_actual'      : y_scaler.inverse_transform(y_actual),\n",
    "})\n",
    "\n",
    "result_df['price_diff'] = result_df['price_prediction'] - result_df['price_actual']\n",
    "result_df.round(2).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But all those separate fit()s and transform()s are exhausting and I'm lazy. Isn't there an easier way to do this?\n",
    "\n",
    "Yup! Because all sklearn algorithms have roughly the same API, you can chain them and operate on them as a group. This is what pipelines are for.\n",
    "\n",
    "* [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "* [Pipeline User Guide](http://scikit-learn.org/stable/modules/pipeline.html#pipeline)\n",
    "\n",
    "Note: most of the main pages for models like the first bullet above have links to the Sklearn User Guide, which gives concrete examples. The documentation is good. Use it.\n",
    "\n",
    "A pipeline is made up of a series of steps that are done sequentially. In practice, this is basically just a list of tuples, with the first element being the step's identifier, and the second element being the model or transformer you want to add to the pipeline. \n",
    "\n",
    "For example, it's pretty common to want to scale something, do PCA, and then fit a model. Let's do this via a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a scaler, PCA, support vector machine classifier.\n",
    "pipe = Pipeline([\n",
    "    ('scale', RobustScaler()),\n",
    "    ('pca'  , PCA()),\n",
    "    ('clf'  , SVR(kernel='linear'))\n",
    "])\n",
    "\n",
    "# This means we can just\n",
    "pipe.fit(X, y.values.ravel())\n",
    "pipe.predict(X.iloc[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isn't that easier? We can also add more complex steps into the pipeline using the FunctionTransformers we discussed before.\n",
    "\n",
    "From the sklearn documentation, we get a function that basically chops off the first column of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def all_but_first_column(input_X):\n",
    "    return input_X[:, 1:]\n",
    "\n",
    "# Which we can wrap in a Function Transformer, and then wrap in a tuple before inserting it\n",
    "# as a pipeline step.\n",
    "('func_trans', FunctionTransformer(all_but_first_column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where this is the most useful is hyperparameter searches.\n",
    "\n",
    "We can use these pipelines to to help us exhaustively run through hyperparameters and to choose the best outcome and parameters. We simply use the following notation with the step name ('stepname') and keyword argument ('kwarg') separated by a double underscore, and then follwed with a list of values like this:\n",
    "\n",
    "    {\n",
    "        'stepname1__kwarg1: [value1, value2],\n",
    "        'stepname2__kwarg2: [value3, value4]\n",
    "    }\n",
    "\n",
    "An example might be helpful. Say we want to test our pipeline above, but we're not sure if a linear kernel or a radial basis function kernel would be preferable for our classifier. Say we also want to tweak the quantile range to see if that helps.\n",
    "\n",
    "Note: this tests every possible permutation, which can take a really, really long time. For example, the example below has to fit 4 pipelines instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid we want to search.\n",
    "grid = {\n",
    "    'scale__quantile_range': [(10.0, 90.0), (25.0, 75.0)],\n",
    "    'clf__kernel'          : ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "\n",
    "# Define the parameters for our search.\n",
    "best_model = GridSearchCV(\n",
    "    pipe, \n",
    "    n_jobs=2, \n",
    "    param_grid=grid, \n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Fit the model ... we are chopping down the data for the sake of time.\n",
    "best_model.fit(X.iloc[:1000,:], y.values.ravel()[:1000])\n",
    "\n",
    "# The model will now have the optimal parameters.\n",
    "print('The best parameters are: {}'.format(best_model.best_params_))\n",
    "\n",
    "# And we can predict with the model.\n",
    "best_model.predict(X.iloc[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## So what algorithm should I use for what?\n",
    "\n",
    "#### TL;DR: \n",
    "\n",
    "¯\\\\\\_(ツ)\\_/¯\n",
    "\n",
    "#### Seriously though:\n",
    "\n",
    "Entire books have been devoted to this topic (see Notebook 7, Resources). You really should ask someone who knows what they are doing (and if you know what you're doing, you should put together a monthly session that exams a particular algorithm, it's strengths, and how to best use it). That said, some of the algorithms available to you are below.\n",
    "\n",
    "Note: if a class states that it is a \"Classifier\" (e.g. Support Vector Classifier), it will have an associated regressor. As previously mentioned, you will use the classifier for discrete targets (e.g. \"setosa\") and regressors for continuous targets (e.g. 6.25).\n",
    "\n",
    "#### Algorithms:\n",
    "\n",
    "* **Lasso Regression**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "    * <a href=\"https://en.wikipedia.org/wiki/Lasso_(statistics)\">Wikipedia</a>\n",
    "    \n",
    "    \n",
    "* **Elastic Net**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/linear_model.html#elastic-net)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Elastic_net_regularization)\n",
    "\n",
    "\n",
    "* **Logistic Regression**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "  \n",
    "  \n",
    "* **Stoochastic Gradient Descent (SVM-based)**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "  \n",
    "  \n",
    "* **Perceptron**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/linear_model.html#perceptron)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Perceptron)\n",
    "  \n",
    "  \n",
    "* **Support Vector Machines**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/svm.html)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "  \n",
    "  \n",
    "* **K-Nearest Neighbors**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/neighbors.html)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "\n",
    "\n",
    "* **Naive Bayes**\n",
    "    * [Sklearn Class Documentation]()\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "  \n",
    "  \n",
    "* **Random Forests**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/ensemble.html#random-forests)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "\n",
    "* **Neural Networks / Multi-Layer Perceptrons**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Neural_network)\n",
    "  \n",
    "  \n",
    "* **Gradient Boosted Trees**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting)\n",
    "\n",
    "\n",
    "* **Clustering**\n",
    "    * [Sklearn Class Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "    * [Sklearn Algorithm Info](http://scikit-learn.org/stable/modules/clustering.html)\n",
    "    * [Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still have no idea, just use this picture:\n",
    "\n",
    "<img src='static/ml_map.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Learing Resources\n",
    "\n",
    "* ### [Sklearn Supervised Learning Model Guide](http://scikit-learn.org/stable/supervised_learning.html)\n",
    "* ### [Sklearn Unsupervised Learning Model Guide](http://scikit-learn.org/stable/unsupervised_learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Up: [Validating](5_validating.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"margin-left: 0;\" src=\"static/roc_curve.svg\" width=\"20%\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:ROC_curves_colors.svg'>נדב ס</a> under the <a href='https://creativecommons.org/licenses/by-sa/4.0/deed.en'>CC BY-SA 4.0</a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
