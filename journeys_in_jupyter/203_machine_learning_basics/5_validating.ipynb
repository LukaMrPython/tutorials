{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://github.com/theonaunheim\">\n",
    "    <img style=\"border-radius: 100%; float: right;\" src=\"static/strawberry_thief_square.png\" width=10% alt=\"Theo Naunheim's Github\">\n",
    "</a>\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "<h1 align='center'>Validating</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: table; width: 100%\">\n",
    "    <div style=\"display: table-row; width: 100%;\">\n",
    "        <div style=\"display: table-cell; width: 50%; vertical-align: middle;\">\n",
    "            <img src=\"static/roc_curve.svg\" width=\"300\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 10%\">\n",
    "        </div>\n",
    "        <br>\n",
    "        <br>\n",
    "        <div style=\"display: table-cell; width: 40%; vertical-align: top;\">\n",
    "            <blockquote>\n",
    "                <p style=\"font-style: italic;\">\"It doesn't matter how beautiful your theory is. It doesn't matter how smart you are. If it doesn't agree with the experiment, it is wrong.\"</p>\n",
    "                <br>\n",
    "                <p>-Richard Feynman</p>\n",
    "            </blockquote>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:ROC_curves_colors.svg'>נדב ס</a> under the <a href='https://creativecommons.org/licenses/by-sa/4.0/deed.en'>CC BY-SA 4.0</a>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generally\n",
    "\n",
    "Model **validation** is the process of taking our model and evaluating its effectiveness. There are a multitude of different ways to validate our classifers and regressors, but most involve creating a **score** that tells us how good or bad our model is. These scores and helper tools are found within the [sklearn.metrics module](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics), and a list of the different scoring types are found on this [table](http://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scoring Types\n",
    "\n",
    "The more common scores you will see are:\n",
    "\n",
    "* **Classification**\n",
    "\n",
    "    * **accuracy**\n",
    "        * [Sklearn Documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score)\n",
    "        * [Sklearn Class Docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "        \n",
    "    * **f1**\n",
    "        * [Sklearn Documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)\n",
    "        * [Sklearn Class Docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)\n",
    "        * [Wikipedia](https://en.wikipedia.org/wiki/F1_score)\n",
    "        \n",
    "    * **precision**\n",
    "        * [Sklearn Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n",
    "        * [Sklearn Class Docs](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)\n",
    "        * [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall#Precision)\n",
    "        \n",
    "    * **recall**\n",
    "        * [Sklearn Documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)\n",
    "        * [Sklearn Class Docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "        * [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall#Recall)\n",
    "        \n",
    "\n",
    "* **Regression**\n",
    "\n",
    "    * **neg_mean_absolute_error**\n",
    "        * [Sklearn Documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error)\n",
    "        * [Sklearn Class Docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)\n",
    "        * [Wikipedia](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "        \n",
    "    * **neg_mean_squared_error**\n",
    "        * [Sklearn Documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error)\n",
    "        * [Sklearn Class Docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)\n",
    "        \n",
    "    * **neg_mean_squared_log_error**\n",
    "        * [Sklearn Documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error)\n",
    "        * [Sklearn Class Docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Which score should I use?\n",
    "\n",
    "It depends.\n",
    "\n",
    "## Depends on what?\n",
    "\n",
    "Fancy stuff outside the scope of this presentation.\n",
    "\n",
    "If you have no idea where to start, I've found f1 is generally pretty good for classifications and negative mean squared error is generally pretty good for regressors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "\n",
    "Generally you have the option of either using your scoring functions directly on your data, or using them as part of a **cross-validator**, which we will describe here.\n",
    "\n",
    "Remember the **train_test_split()** from before that we used to prevent **overfitting**? In practice, that meant that we were only giving our model a portion of the data we have to train. Wouldn't it be useful if we could use all that data, but just rotate our training and testing sets?\n",
    "\n",
    "That's what [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) is. See also the <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\">Wikipedia page</a>.\n",
    "\n",
    "<img src='static/cross_validation.jpg'>\n",
    "\n",
    "Image courtesy of <a href=\"https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.jpg\">Fabian Flöck</a> under the <a href=\"https://creativecommons.org/licenses/by-sa/3.0/deed.en\">CC BY-SA 3.0</a>.\n",
    "\n",
    "Note: we already did this using the GridSearchCV. The CV stands for \"cross-validation\".\n",
    "\n",
    "#### Direct Use\n",
    "\n",
    "Say we want to evaluate our Iris dataset again and we want to compare two models.\n",
    "\n",
    "The first of these models will use 'sepal_length' and 'sepal_width' fed through a radial basis function SVM.\n",
    "\n",
    "The second of these models will use 'petal_length' and 'petal_width' fed through a linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "# Split our data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:4], df.iloc[:,4])\n",
    "X1_train = X_train[['sepal_length', 'sepal_width']]\n",
    "X1_test  = X_test[['sepal_length', 'sepal_width']]\n",
    "X2_train = X_train[['petal_length', 'petal_width']]\n",
    "X2_test  = X_test[['petal_length', 'petal_width']]\n",
    "y_train  = y_train\n",
    "actual   = y_test\n",
    "\n",
    "# Create our pipelines\n",
    "pipe_1 = Pipeline([('scale', RobustScaler()), ('clf'  , SVC(kernel='rbf'))])\n",
    "pipe_2 = Pipeline([('scale', RobustScaler()), ('clf'  , SVC(kernel='linear'))])\n",
    "\n",
    "# Fit our pipelines\n",
    "pipe_1.fit(X1_train, y_train)\n",
    "pipe_2.fit(X2_train, y_train)\n",
    "\n",
    "# Predict our outcomes\n",
    "pipe_1_predictions = pipe_1.predict(X1_test)\n",
    "pipe_2_predictions = pipe_2.predict(X2_test)\n",
    "\n",
    "# Present our findings\n",
    "results_df = pd.DataFrame({\n",
    "    'pipe_1_prediction': pipe_1_predictions,\n",
    "    'pipe_2_prediction': pipe_2_predictions, \n",
    "    'actual'           : actual\n",
    "})\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then use pass y_true and y_predicted to get an accuracy and f1 score for each model.\n",
    "scores = {\n",
    "    'pipe_1_accuracy': accuracy_score(actual, pipe_1_predictions),\n",
    "    'pipe_2_accuracy': accuracy_score(actual, pipe_2_predictions),\n",
    "    # Note, multiclass f1 scores need average set to None (default is binary)\n",
    "    'pipe_1_f1'      : f1_score(actual, pipe_1_predictions, average='macro'),\n",
    "    'pipe_2_f1'      : f1_score(actual, pipe_2_predictions, average='macro')\n",
    "}\n",
    "\n",
    "for key, value in scores.items():\n",
    "    print(key)\n",
    "    print(value.round(4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Cross-Validation\n",
    "\n",
    "Cross-validation is even easier than doing it manually. To cross-validate, we use the cleverely named [cross_val_score() function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). Instead of manually fitting and splitting and analyzing, we can just do it [all at once](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n",
    "\n",
    "It takes longer complexity, but better results because more data. You can fine tune the number of splits with the 'cv' parameter, which takes the number of splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "X1 = df[['sepal_length', 'sepal_width']]\n",
    "X2 = df[['petal_length', 'petal_width']]\n",
    "y  = df['species']\n",
    "\n",
    "# Create our pipelines\n",
    "pipe_1 = Pipeline([('scale', RobustScaler()), ('clf', SVC(kernel='rbf'))])\n",
    "pipe_2 = Pipeline([('scale', RobustScaler()), ('clf', SVC(kernel='linear'))])\n",
    "\n",
    "# Run our 5-fold cross validation and get the average score of the 5.\n",
    "# Note: because this is multiclass, we need to use f1_micro, f1_macro, weighted (see documentation)\n",
    "# Note: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "pipe_1_f1  = cross_val_score(pipe_1, X1, y, scoring='f1_macro', cv=5).mean()\n",
    "pipe_1_acc = cross_val_score(pipe_1, X1, y, scoring='accuracy', cv=5).mean()\n",
    "pipe_2_f1  = cross_val_score(pipe_2, X2, y, scoring='f1_macro', cv=5).mean()\n",
    "pipe_2_acc = cross_val_score(pipe_2, X2, y, scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scores out\n",
    "scores = {\n",
    "    'pipe_1_f1' : pipe_1_f1,\n",
    "    'pipe_2_f1' : pipe_2_f1,\n",
    "    'pipe_1_acc': pipe_1_acc,\n",
    "    'pipe_2_acc': pipe_2_acc,\n",
    "}\n",
    "\n",
    "for key, value in scores.items():\n",
    "    print(key)\n",
    "    print(value.round(4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Validation\n",
    "\n",
    "Sklearn also has a bunch of helper functions to make your life easier. Most useful of these will be your classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking our results df from above\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classification report\n",
    "\n",
    "This provides you with quick and dirty summary of your model results.\n",
    "\n",
    "* [classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_1 = classification_report(\n",
    "    results_df.actual,\n",
    "    results_df.pipe_1_prediction\n",
    ")\n",
    "\n",
    "report_2 = classification_report(\n",
    "    results_df.actual,\n",
    "    results_df.pipe_2_prediction\n",
    ")\n",
    "\n",
    "print('Pipe 1:\\n', report_1, '\\n\\n\\n', 'Pipe 2:\\n', report_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "This basically allows you to see where you correctly predicted the results and where you did not. It has your predicted class on the horizontal and your actual class on the vertical.\n",
    "\n",
    "<table>\n",
    "<tbody><tr>\n",
    "<th style=\"background: white; border: currentColor; border-image: none;\" rowspan=\"2\" colspan=\"2\">\n",
    "</th>\n",
    "<th style=\"background: none;\" colspan=\"3\">Actual class\n",
    "</th></tr>\n",
    "<tr>\n",
    "<th>Cat\n",
    "</th>\n",
    "<th>Dog\n",
    "</th>\n",
    "<th>Rabbit\n",
    "</th></tr>\n",
    "<tr>\n",
    "<th style=\"height: 6em;\" rowspan=\"3\"><div style=\"display: inline-block; transform: rotate(-90deg); -webkit-transform: rotate(-90deg);\">Predicted<br> class</div>\n",
    "</th>\n",
    "<th>Cat\n",
    "</th>\n",
    "<td><b>5</b>\n",
    "</td>\n",
    "<td>2\n",
    "</td>\n",
    "<td>0\n",
    "</td></tr>\n",
    "<tr>\n",
    "<th>Dog\n",
    "</th>\n",
    "<td>3\n",
    "</td>\n",
    "<td><b>3</b>\n",
    "</td>\n",
    "<td>2\n",
    "</td></tr>\n",
    "<tr>\n",
    "<th>Rabbit\n",
    "</th>\n",
    "<td>0\n",
    "</td>\n",
    "<td>1\n",
    "</td>\n",
    "<td><b>11</b>\n",
    "</td></tr></tbody></table>\n",
    "\n",
    "\n",
    "* [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "* [confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "Note: this example also uses seaborn's [heatmap](http://seaborn.pydata.org/generated/seaborn.heatmap.html) to make the output shiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = ['versicolor', 'virginica', 'setosa']\n",
    "\n",
    "cm1 = confusion_matrix(\n",
    "    results_df.actual,\n",
    "    results_df.pipe_1_prediction,\n",
    "    labels=species\n",
    ")\n",
    "\n",
    "cm2 = confusion_matrix(\n",
    "    results_df.actual,\n",
    "    results_df.pipe_2_prediction,\n",
    "    labels=species\n",
    ")\n",
    "\n",
    "print(cm1)\n",
    "print()\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm1, annot=True, xticklabels=species, yticklabels=species, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm2, annot=True, xticklabels=species, yticklabels=species, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You may also want to take a look at receiver operating characteristic curves.\n",
    "\n",
    "* [Sklearn Docucumentation](http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc)\n",
    "* [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Learing Resources\n",
    "\n",
    "* ### [Sklearn Model Selection and Scoring](http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html)\n",
    "* ### [Sklearn Model Evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "* ### [Sklearn Cross-Validation](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "* ### [Sklearn Scoring Table](http://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Up: [Resources](6_resources.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"margin-left: 0;\" src=\"static/gravity_well.svg\" width=\"20%\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:Gravity_well_plot.svg'>BenRG</a>; released into the public domain.\n",
    "</div>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
