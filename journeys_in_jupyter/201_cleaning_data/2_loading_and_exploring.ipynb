{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://github.com/theonaunheim\">\n",
    "    <img style=\"border-radius: 100%; float: right;\" src=\"static/strawberry_thief_square.png\" width=10% alt=\"Theo Naunheim's Github\">\n",
    "</a>\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "<h1 align='center'>Loading and Exploring</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: table; width: 100%\">\n",
    "    <div style=\"display: table-row; width: 100%;\">\n",
    "        <div style=\"display: table-cell; width: 50%; vertical-align: middle;\">\n",
    "            <img src=\"static/loading.png\" width=\"300\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 10%\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 40%; vertical-align: top;\">\n",
    "            <blockquote>\n",
    "                <p style=\"font-style: italic;\">“It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts.”</p>\n",
    "                <br>\n",
    "                <p>— Sherlock Holmes</p>\n",
    "                <br>\n",
    "            </blockquote>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "\n",
    "Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:New_Zealand_RP-7.1_(right).svg'>The New Zealand Transport Authority</a>, released into the public domain.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generally\n",
    "\n",
    "Like most things, loading data in Python goes much more smoothly if you can get it right the first time. Consequently, we are going to focus on mechanisms for loading data first, and then later work on how to _munge_ that data.\n",
    "\n",
    "> “**Data wrangling**, sometimes referred to as **data munging**, is the process of transforming and mapping data from one ‘raw’ data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.”¹\n",
    "\n",
    "Pandas has a large number of  [supported input types](https://pandas.pydata.org/pandas-docs/stable/io.html), but most of our work will be done through the workhouse [read_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv) function, the accompanying [read_excel()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html) function, and assorted [DataFrame construction methods](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html).\n",
    "\n",
    "---\n",
    "¹Wikipedia contributors, “Data wrangling,” Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Data_wrangling&oldid=834062041 (accessed May 16, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Modules covered\n",
    "\n",
    "### Standard Library\n",
    "* [csv](https://docs.python.org/3.4/library/csv.html)\n",
    "* [pathlib](https://docs.python.org/3/library/pathlib.html)\n",
    "\n",
    "### Third-Party Libraries\n",
    "* [chardet](http://chardet.readthedocs.io/en/latest/usage.html)\n",
    "* [numpy](https://docs.scipy.org/doc/numpy-1.13.0/reference/)\n",
    "* [pandas](https://pandas.pydata.org/)\n",
    "* [tabulate](https://tabulate.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "# Modules not covered\n",
    "\n",
    "### Standard Library\n",
    "* None\n",
    "\n",
    "### Third-Party Libraries\n",
    "* None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib imports\n",
    "import csv\n",
    "import pathlib\n",
    "\n",
    "# Third-party imports\n",
    "import chardet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wide, Wonderful, Wacky World of Filetypes\n",
    "\n",
    "Sometimes people have access to conformant CSV files, well-structured databases, and well-kept Excel spreadsheets.\n",
    "\n",
    "And sometimes, you get a trainwreck of a database or streams of bytes that want you to believe they’re data, but in reality just really wanna ruin your day…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed-width files\n",
    "\n",
    "Fixed-width files are handled with [read_fwf()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_fwf.html).  You can infer columns if the file is well-structured, or specify their size with the 'widths' keyword argument. You can infer headers or specify the non-existance of headers with headers=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify fixed-width file path\n",
    "IRIS_FWF_PATH = './data/iris_dataset.txt'\n",
    "\n",
    "# Sometimes people send you fixed width files...\n",
    "with open(IRIS_FWF_PATH) as f:\n",
    "    print(f.read(256))\n",
    "\n",
    "# ... and you can read these with pd.read_fwf()\n",
    "df = pd.read_fwf(IRIS_FWF_PATH, colspecs='infer', widths=None)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Franken-CSVs\n",
    "\n",
    "Sometimes CSVs are not really CSVs. If someone sends you a janky CSV with weird separators and line breaks, Pandas can handle it with read_csv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify frankencsv path.\n",
    "FRANKEN_CSV_PATH = './data/website_feedback.csv'\n",
    "\n",
    "# Sometimes people send you weird files.\n",
    "with open(FRANKEN_CSV_PATH) as f:\n",
    "    print(f.read(500))\n",
    "\n",
    "# You can read these with pd.read_csv()\n",
    "df = pd.read_csv(\n",
    "    FRANKEN_CSV_PATH,\n",
    "    sep=';',\n",
    "    # Only the 'sep' argument is needed, the others are just for show.\n",
    "    index_col='title',\n",
    "    usecols=[0, 1, 2, 3, 4, 5],\n",
    "    dtype={'category': 'category'},\n",
    "    converters={'satistifaction': pd.to_numeric},\n",
    "    thousands=',',\n",
    "    na_values='no',\n",
    "    keep_default_na=True,\n",
    "    quotechar='\"',\n",
    "    escapechar=None,\n",
    "    encoding='utf8',\n",
    "    doublequote=True,\n",
    "    delimiter=None,\n",
    "    lineterminator=None,\n",
    "    error_bad_lines=True,\n",
    "    skip_blank_lines=False,\n",
    ")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas.read_csv() signature\n",
    "\n",
    "pandas.read_csv(filepath_or_buffer, sep=', ', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='\"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=None, compact_ints=None, use_unsigned=None, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None)\n",
    "\n",
    "Can you remember all that? Me neither. [Read the documentation!](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv) That’s what it’s there for!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel files\n",
    "\n",
    "For better or worse, people use a lot of Excel. Pandas supports that (even if I personally **do not**) via [read_excel()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Excel doc path\n",
    "EXCEL_PATH = './data/greenway_analysis.xlsx'\n",
    "\n",
    "# Sometimes people send you weird files.\n",
    "with open(EXCEL_PATH, 'rb') as f:\n",
    "    print(f.read(100))\n",
    "\n",
    "# You can read these with pd.read_csv()\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read_excel() signature\n",
    "\n",
    "pandas.read_excel(io, sheet_name=0, header=0, skiprows=None, skip_footer=0, index_col=None, names=None, usecols=None, parse_dates=False, date_parser=None, na_values=None, thousands=None, convert_float=True, converters=None, dtype=None, true_values=None, false_values=None, engine=None, squeeze=False, **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Warning: opening a CSV with read_excel will fail.\n",
    "pd.read_excel('./data/requests-for-open-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Warning: opening an Excel with read_csv will fail.\n",
    "pd.read_csv('./data/greenway_analysis.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other types\n",
    "\n",
    "Remember, if you run into difficulty, see the supported types or build something from scratch.\n",
    "\n",
    "* pd.read_clipboard()\n",
    "* pd.read_gbq()\n",
    "* pd.read_hdf()\n",
    "* pd.read_html()\n",
    "* pd.read_json()\n",
    "* pd.read_sas()\n",
    "* pd.read_sql()\n",
    "* pd.read_stata()\n",
    "* pd.read_table()\n",
    "\n",
    "And you can also build dataframes from scratch with raw data.\n",
    "\n",
    "* pd.DataFrame()\n",
    "* pd.DataFrame.from_dict()\n",
    "* pd.DataFrame.from_records()\n",
    "* pd.DataFrame.from_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Nbr': np.random.normal(1, 3, size=3),\n",
    "    'Msg': ['such dataframe', 'very elegant', 'wow']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong encodings\n",
    "\n",
    "In olden times, you didn't have to worry about [character encoding](https://en.wikipedia.org/wiki/Character_encoding), which you can think of as the manner in which bytes translate to characters. You had [ASCII](https://en.wikipedia.org/wiki/ASCII) (or if you were particularly unlucky, [EBCDIC](https://en.wikipedia.org/wiki/EBCDIC)), and you didn't have to convert things back and forth.\n",
    "\n",
    "We are not so lucky these days, however, you have tools to deal with it. Common encodings you will come across are 'cp1252' (Windows), 'utf8' (everywhere), 'ascii' (old, but good), and 'latin-1' (evil). A [list of encodings supported out of the box in Python are here](https://docs.python.org/3/library/codecs.html#standard-encodings)\n",
    "\n",
    "Note: both 'read_csv()' and 'to_csv()' give you the option to save or read whatever encoding you please. The default is utf8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default encoding is utf8\n",
    "df = pd.read_csv('./data/weather.csv', encoding='utf8')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which can cause other encodings to file\n",
    "df = pd.read_csv('./data/weather.csv', encoding='cp1252')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which can cause other encodings to file\n",
    "df = pd.read_csv('./data/weather.csv', encoding='ascii')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we determine encodings?\n",
    "\n",
    "Your first option is to always ensure you are loading from and saving to the default encoding, UTF-8. Alternatively, you can use chardet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all our CSVs.\n",
    "cwd = pathlib.Path().cwd()\n",
    "files = list(cwd.rglob('**/*.csv'))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# For each CSV\n",
    "for csv_file in files:\n",
    "    # Read CSV and read encoding with chardet\n",
    "    with open(csv_file, 'rb') as f:\n",
    "        encoding = chardet.detect(f.read())\n",
    "        csv_name = csv_file.name\n",
    "    # Addthe name and encoding info to dataframe\n",
    "    df = df.append({\n",
    "        'filename'  : csv_name,\n",
    "        'encoding'  : encoding['encoding'],\n",
    "        'confidence': encoding['confidence']\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Show frame\n",
    "df[['filename', 'encoding', 'confidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Malformed files\n",
    "\n",
    "The best laid schemes o' mice an' men often result in seriously broken CSV files. Here are a couple common scenarios that a) apply to files as a whole and b) won't be covered in our datatype-specific cleaning later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### No headers\n",
    "\n",
    "Generally pandas expects that you'll have a header to name your column types. If not, no big deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the head function still works no matter what.\n",
    "df = pd.read_csv('./data/headless.csv').head(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the pass headers=0 and specify the headers instead.\n",
    "column_names = [\n",
    "    'OBJECTID', 'COMPLEX_NAME', 'ORGANIZATION_NAME', 'UNIT_NAME', 'UNIT_TYPE', \n",
    "    'INTEREST_TYPE1', 'INTEREST_TYPE2', 'ACRES', 'SHAPE.AREA', 'SHAPE.LEN'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    './data/headless.csv',\n",
    "    header=None,\n",
    "    names=column_names\n",
    ")\n",
    "\n",
    "# Alternatively\n",
    "# df = pd.read_csv('./data/headless.csv', header=None)\n",
    "# df.columns = column_names\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Duplicate data\n",
    "\n",
    "Often times you will have duplicate data. This can be easily rectified on a per record or per column basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/complex_dupe.csv')\n",
    "devils_lake = df[df['COMPLEX_NAME'] == 'DEVILS LAKE WMD']\n",
    "\n",
    "devils_lake.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can eliminate exact row duplicates\n",
    "devils_lake.drop_duplicates().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or for a column or set of specific columns\n",
    "devils_lake.drop_duplicates(subset=['COMPLEX_NAME', 'ORGANIZATION_NAME']).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Unnecessary data\n",
    "\n",
    "Don't load what you don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/complex_dupe.csv', usecols=['OBJECTID','COMPLEX_NAME', 'ORGANIZATION_NAME'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Garbage\n",
    "\n",
    "Sometimes you have garbage values for no particular reason. You can manually pull those out, or you can use convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/weather.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().replace(to_replace='ツ', value='._.', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You an also find the lines for specific garbage using this idiom\n",
    "df[(df == r'¯\\_(?)_/¯').any(axis=1)]\n",
    "\n",
    "# # NOT REQUIRED, but parsing this out ...\n",
    "# # Get whether each cell is equal to the value\n",
    "# eq_to_df = df == r'¯\\_(?)_/¯'\n",
    "# # Get whether each row contains a cell equal to the value\n",
    "# row_contains_bool = eq_to_df.any(axis=1)\n",
    "# # Get those rows where the condition is true.\n",
    "# df_containing_bad_vals = df[row_contains]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Absurdly large files\n",
    "\n",
    "Python is generally pretty efficient when it comes to packing a lot of data into a very small space. That said, RAM can be a limitation. If you cannot fit your file into RAM, you have a variety of options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking (preferred)\n",
    "\n",
    "You can break off bits of a dataframe at a time using the \"chunksize\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_count = 0\n",
    "\n",
    "for chunk in pd.read_csv('./data/weather.csv', chunksize=100):\n",
    "    chunk_count += 1\n",
    "    print('The first cell in chunk {} is {}'.format(chunk_count, chunk.iloc[0,0]))\n",
    "\n",
    "chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Offset / nrows\n",
    "\n",
    "You can read a limited number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./data/weather.csv', nrows=100, skiprows=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-core\n",
    "\n",
    "There are a variety of other tactics, such as using third party libraries for lazy evalution (dask), using disk/dbs as cache, or key ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring\n",
    "\n",
    "There are books devoted to exploratory data analysis (EDA). Here are some quick tips you might use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./data/greenway_analysis.xlsx')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, the return values are regular Python objects, so you can do whatever.\n",
    "greenway_names = df['main_greenway'].unique()\n",
    "path_names = [item for item in greenway_names if 'path' in item.lower()]\n",
    "path_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes: You can check a dataframe or series dtype with .dtypes\n",
    "df.dtypes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe will give you information about the numeric types.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can get these individually.\n",
    "df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or examine the compnent column series individually\n",
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info will give you information about the values stored.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use value counts to examine columns.\n",
    "df['greenway'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get correlation, etc.\n",
    "df = pd.read_fwf(IRIS_FWF_PATH, colspecs='infer', widths=None)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which as an aside looks way cooler as a scatter matrix\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "pd.tools.plotting.scatter_matrix(df, figsize=(7,7), diagonal='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Learing Resources\n",
    "\n",
    "* ### [10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "* ### [Pandas Tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html)\n",
    "* ### [Pandas Cookbook](https://pandas.pydata.org/pandas-docs/stable/tutorials.html#pandas-cookbook)\n",
    "* ### [Pandas Guide for New Users](https://pandas.pydata.org/pandas-docs/stable/tutorials.html#lessons-for-new-pandas-users)\n",
    "* ### [Pandas Intro to Data Structures](https://pandas.pydata.org/pandas-docs/stable/dsintro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Up: [Missing Data](3_missing_data.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"margin-left: 0;\" src=\"static/empty_set.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:Empty_set.svg'>Octahedron80</a>, released into the public domain\n",
    "</div>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
