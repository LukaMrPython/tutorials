{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://github.com/theonaunheim\">\n",
    "    <img style=\"border-radius: 100%; float: right;\" src=\"static/strawberry_thief_square.png\" width=10% alt=\"Theo Naunheim's Github\">\n",
    "</a>\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<h1 align='center'>Other</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: table; width: 100%\">\n",
    "    <div style=\"display: table-row; width: 100%;\">\n",
    "        <div style=\"display: table-cell; width: 50%; vertical-align: middle;\">\n",
    "            <img src=\"static/other.png\" width=\"400\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 10%\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 40%; vertical-align: top;\">\n",
    "            <blockquote>\n",
    "                <p style=\"font-style: italic;\">'\"This entire thing is the quote, not just the part in quote marks.\" [Quote marks, brackets, and editor's note are all in the original. -Ed.]'</p>\n",
    "                <br>\n",
    "                <p>â€” xkcd</p>\n",
    "            </blockquote>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    <br>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:Flag_of_None.svg'>Rainer Zenz</a>. Image is public domain.\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other\n",
    "\n",
    "While we have endeavored to give you a high-level overview of the tools available for cleaning data, this is only really scratching the surface. In addition, there are tools and techniques that do not fit neatly into the previous categories explored, so we speak to them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Modules covered\n",
    "\n",
    "### Standard Library\n",
    "* [csv](https://docs.python.org/3/library/csv.html)\n",
    "\n",
    "\n",
    "### Third-Party Libraries\n",
    "* [pandas](https://pandas.pydata.org/)\n",
    "* [dask](https://dask.pydata.org/en/latest)\n",
    "\n",
    "\n",
    "# Modules not covered\n",
    "\n",
    "### Standard Library\n",
    "* A whole bunch\n",
    "\n",
    "### Third-Party Libraries\n",
    "* A whole bunch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python stdlib imports\n",
    "import csv\n",
    "\n",
    "# Third party imports\n",
    "import dask.dataframe\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies for dealing with ginormous data\n",
    "\n",
    "By default, pandas does not load data lazily because it is less convenient and focuses on performing computation in-memory on arrays, Series, and DataFrames. At times, however, your data will be too large to fit in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'Categories' is akin to using a smaller surogate key\n",
    "names = pd.read_fwf('./data/iris_dataset.txt')['class']\n",
    "names_cat = names.astype('category')\n",
    "\n",
    "names_mu     = names.memory_usage()\n",
    "names_cat_mu = names_cat.memory_usage()\n",
    "multiplier   = names_mu / names_cat_mu\n",
    "print('In this case, the strings use {} times more space than a categorical.'.format(multiplier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do chunked read\n",
    "for chunk in pd.read_csv('./data/ascii_table.csv', chunksize=1024):\n",
    "    # process chunk\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to chunked read, it's even faster to use the CSV module for a single column.\n",
    "with open('./data/ascii_table.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data_header = next(reader)[0]\n",
    "    for remaining_row in reader:\n",
    "        # Process as needed lazily.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask helps with lazy computation but we will need to explicitly told to compute()\n",
    "dd = dask.dataframe.read_csv('./data/ascii_table.csv')\n",
    "decimal_max = dd['decimal_value'].max().compute()\n",
    "decimal_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other easy preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_fwf('./data/iris_dataset.txt')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sum is useful.\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'original'      : df.sepal_length_cm,\n",
    "        'cumulative_sum': df.sepal_length_cm.cumsum(),\n",
    "    }\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff and pct_change is useful for differences\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'original'   : df.sepal_length_cm,\n",
    "        'diff'       : df.sepal_length_cm.diff(),\n",
    "        'pct_change' : df.sepal_length_cm.pct_change(), \n",
    "    }\n",
    ").head().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The T property on a pandas DataFrame transposes the data (reflects over its diagonal)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank gives you ranks.\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'sepal_orig'     : df.head(5).sepal_length_cm,\n",
    "        'sepal_rank'     : df.head(5).sepal_length_cm.rank(ascending=True),\n",
    "        'sepal_pct_rank' : df.head(5).sepal_length_cm.rank(ascending=True, pct=True),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift allows you to shift columns back and forth.\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'sepal_length_cm'       : df.sepal_length_cm,\n",
    "        'three_observations_ago': df.sepal_length_cm.shift(3), # pos or neg\n",
    "        'diff'                  : df.sepal_length_cm - df.sepal_length_cm.shift(3),\n",
    "    }\n",
    ")[['sepal_length_cm', 'three_observations_ago', 'diff']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Python Data Analysis tools for merge and concatenate information.\n",
    "pd.concat(\n",
    "    [\n",
    "        pd.merge(\n",
    "            pd.DataFrame({'a': [1,5,3], 'b': [4,5,6]}),\n",
    "            pd.DataFrame({'a': [3,2,1], 'c': [0,9,8]}),   \n",
    "            how='left'\n",
    "        ),\n",
    "        pd.Series(['d','d','d'], name='d')\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, change to string formatted numbers to strings just before export.\n",
    "pd.Series([1000000.29, 400000.99, 41112.38]).map(lambda x: '{:,.2f}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful NumPy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where gets you the indicies of particular conditions\n",
    "TARGET_VALUE = 5.5\n",
    "x_vals, y_vals = np.where(df.select_dtypes([np.float64]) == TARGET_VALUE)\n",
    "\n",
    "indices = df.index[x_vals].values\n",
    "columns = df.columns[y_vals].values\n",
    "\n",
    "for index, column in zip(indices, columns):\n",
    "    print('Sample {} is equal to {} in column {}'.format(index, TARGET_VALUE, column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argsort is used to rank by sort.\n",
    "list_to_sort = ['a', 'c', 'b', 'd']\n",
    "indices = np.argsort(list_to_sort) + 1\n",
    "\n",
    "for letter, index in zip(list_to_sort, indices):\n",
    "    print('Letter {} will be item {} in a sorted list.'.format(letter, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Learning Resources\n",
    "\n",
    "* None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Questions Before Exercises?\n",
    "\n",
    "\n",
    "# Next Up: [Cleaning Exercises](6_cleaning_exercises.ipynb)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
